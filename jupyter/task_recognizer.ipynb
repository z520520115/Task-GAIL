{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path and Parameter Settings\n",
    "- Path: Datasets and Models\n",
    "- Parameter: CNN, LSTM, Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "data_path = \"./data/task04/\"\n",
    "save_model_path = \"./crnn_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn architecture\n",
    "CNN_fc_hidden1, CNN_fc_hidden2 = 1024, 768\n",
    "cnn_embed_dim = 512\n",
    "img_x, img_y = 256, 342\n",
    "dropout_p = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm architecture\n",
    "RNN_hidden_layers = 3\n",
    "RNN_hidden_nodes = 512\n",
    "RNN_FC_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "k = 4                   # number of target category\n",
    "epochs = 120\n",
    "batch_size = 1\n",
    "learning_rate = 1e-4\n",
    "log_interval = 10       # interval for displaying training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which frame to begin & end in videos\n",
    "b_frame, e_frame, s_frame = 1, 29, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "- CNN model: A CNN function encodes (meaning compressing dimension) every 2D image into a 1D vector\n",
    "- LSTM model: A RNN receives a sequence input vectors from the CNN encoder and outputs another 1D sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, img_x=90, img_y=120, fc_hidden1=512, fc_hidden2=512, drop_p=0.3, CNN_embed_dim=300):\n",
    "        super(CNN, self).__init__()\n",
    "        self.img_x = img_x\n",
    "        self.img_y = img_y\n",
    "        self.CNN_embed_dim = CNN_embed_dim\n",
    "\n",
    "        # CNN architechtures\n",
    "        self.ch1, self.ch2, self.ch3, self.ch4 = 32, 64, 128, 256\n",
    "        self.k1, self.k2, self.k3, self.k4 = (5, 5), (3, 3), (3, 3), (3, 3)  # 2d kernal size\n",
    "        self.s1, self.s2, self.s3, self.s4 = (2, 2), (2, 2), (2, 2), (2, 2)  # 2d strides\n",
    "        self.pd1, self.pd2, self.pd3, self.pd4 = (0, 0), (0, 0), (0, 0), (0, 0)  # 2d padding\n",
    "\n",
    "        # conv2D output shapes\n",
    "        self.conv1_outshape = conv2D_output_size((self.img_x, self.img_y), self.pd1, self.k1,\n",
    "                                                 self.s1)  # Conv1 output shape\n",
    "        self.conv2_outshape = conv2D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\n",
    "        self.conv3_outshape = conv2D_output_size(self.conv2_outshape, self.pd3, self.k3, self.s3)\n",
    "        self.conv4_outshape = conv2D_output_size(self.conv3_outshape, self.pd4, self.k4, self.s4)\n",
    "\n",
    "        # fully connected layer hidden nodes\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1, padding=self.pd1),\n",
    "            nn.BatchNorm2d(self.ch1, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2,\n",
    "                      padding=self.pd2),\n",
    "            nn.BatchNorm2d(self.ch2, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k3, stride=self.s3,\n",
    "                      padding=self.pd3),\n",
    "            nn.BatchNorm2d(self.ch3, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.ch3, out_channels=self.ch4, kernel_size=self.k4, stride=self.s4,\n",
    "                      padding=self.pd4),\n",
    "            nn.BatchNorm2d(self.ch4, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        self.drop = nn.Dropout2d(self.drop_p)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(self.ch4 * self.conv4_outshape[0] * self.conv4_outshape[1],\n",
    "                             self.fc_hidden1)  # fully connected layer, output k classes\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        self.fc3 = nn.Linear(self.fc_hidden2, self.CNN_embed_dim)  # output = CNN embedding latent variables\n",
    "\n",
    "    def forward(self, x_3d):\n",
    "        cnn_embed_seq = []\n",
    "        for t in range(x_3d.size(1)):\n",
    "            # CNNs\n",
    "            x = self.conv1(x_3d[:, t, :, :, :])\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv3(x)\n",
    "            x = self.conv4(x)\n",
    "            x = x.view(x.size(0), -1)  # flatten the output of conv\n",
    "\n",
    "            # FC layers\n",
    "            x = F.relu(self.fc1(x))\n",
    "            # x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "            x = self.fc3(x)\n",
    "            cnn_embed_seq.append(x)\n",
    "\n",
    "        # swap time and sample dim such that (sample dim, time dim, CNN latent dim)\n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        # cnn_embed_seq: shape=(batch, time_step, input_size)\n",
    "\n",
    "        return cnn_embed_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, CNN_embed_dim=300, h_RNN_layers=3, h_RNN=256, h_FC_dim=128, drop_p=0.3, num_classes=50):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.RNN_input_size = CNN_embed_dim\n",
    "        self.h_RNN_layers = h_RNN_layers  # RNN hidden layers\n",
    "        self.h_RNN = h_RNN  # RNN hidden nodes\n",
    "        self.h_FC_dim = h_FC_dim\n",
    "        self.drop_p = drop_p\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.LSTM = nn.LSTM(\n",
    "            input_size=self.RNN_input_size,\n",
    "            hidden_size=self.h_RNN,\n",
    "            num_layers=h_RNN_layers,\n",
    "            batch_first=True,  # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(self.h_RNN, self.h_FC_dim)\n",
    "        self.fc2 = nn.Linear(self.h_FC_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x_RNN):\n",
    "        self.LSTM.flatten_parameters()\n",
    "        RNN_out, (h_n, h_c) = self.LSTM(x_RNN, None)\n",
    "        \"\"\" h_n shape (n_layers, batch, hidden_size), h_c shape (n_layers, batch, hidden_size) \"\"\"\n",
    "        \"\"\" None represents zero initial hidden state. RNN_out has shape=(batch, time_step, output_size) \"\"\"\n",
    "\n",
    "        # FC layers\n",
    "        x = self.fc1(RNN_out[:, -1, :])  # choose RNN_out at the last time step\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2D_output_size(img_size, padding, kernel_size, stride):\n",
    "    # compute output shape of conv2D\n",
    "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
    "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int))\n",
    "    return outshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading\n",
    "- Load the selected frame in the dataset and transform it into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_CRNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, data_path, folders, labels, frames, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.data_path = data_path\n",
    "        self.labels = labels\n",
    "        self.folders = folders\n",
    "        self.transform = transform\n",
    "        self.frames = frames\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(self.folders)\n",
    "\n",
    "    def read_images(self, path, selected_folder, use_transform):\n",
    "        x = []\n",
    "        for i in self.frames:\n",
    "            image = Image.open(os.path.join(path, selected_folder, 'frame{:d}.png'.format(i)))\n",
    "\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "\n",
    "            x.append(image)\n",
    "        x = torch.stack(x, dim=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        # Select sample\n",
    "        folder = self.folders[index]\n",
    "\n",
    "        # Load data\n",
    "        x = self.read_images(self.data_path, folder, self.transform)     # (input) spatial images\n",
    "        y = torch.LongTensor([self.labels[index]])                  # (labels) LongTensor are for int64 instead of FloatTensor\n",
    "\n",
    "        # print(x.shape)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition\n",
    "- Train: Set the training model and parameters to update and display the information\n",
    "- Validation: Set up validated models and processes and save model records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_intreval, model, device, train_loader, optimizer, epoch):\n",
    "    # Set model as training mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.train() # cnn\n",
    "    rnn_decoder.train() # lstm\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0 # counting total training sample in one epoch\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader): \n",
    "        # distribute data to device\n",
    "        x,y = x.to(device), y.to(device).view(-1, ) \n",
    "\n",
    "        N_count += x.size(0)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        output = rnn_decoder(cnn_encoder(x)) # output dim = (batch_size, number of classes)\n",
    "\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        losses.append(loss.item()) \n",
    "\n",
    "        # to compute accurary\n",
    "        y_pred = torch.max(output, 1)[1] \n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), \n",
    "                                    y_pred.cpu().data.squeeze().numpy()) \n",
    "        scores.append(step_score) # computed on GPU\n",
    "\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        # show infotmation\n",
    "        if(batch_idx + 1) % log_intreval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "        return losses, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    cnn_encoder, rnn_decoder = model\n",
    "    cnn_encoder.eval() \n",
    "    rnn_decoder.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad(): \n",
    "        for x, y in test_loader:\n",
    "            # distribute data to device\n",
    "            x, y = x.to(device), y.to(device).view(-1, )\n",
    "\n",
    "            output = rnn_decoder(cnn_encoder(x))\n",
    "\n",
    "            loss = F.cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                  # sum up the batch loss\n",
    "            y_pred = output.max(1, keepdim = True)[1] # (y_pred != output) get the index of the max log-probability\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y) \n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim = 0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100 * test_score))\n",
    "\n",
    "    # save Pytorch model of best record\n",
    "    torch.save(cnn_encoder.state_dict(), os.path.join(save_model_path, 'cnn_epoch{}.pth'.format(epoch + 1)))\n",
    "    torch.save(rnn_decoder.state_dict(), os.path.join(save_model_path, 'lstm_epoch{.pth'.format(epoch + 1)))\n",
    "    torch.save(optimizer.state.dict(), os.path.join(save_model_path, 'optimizer_eopch{}.pth'.format(epoch + 1)))\n",
    "    print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return test_loss, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA and Data Processing\n",
    "- CUDA: Check GPU exists and parallelize model\n",
    "- Data Processing: Find the category of the task and the defined label from the dataset and store it in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect devices\n",
    "use_cuda = torch.cuda.is_available()                 # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\") # use CPU or GPU\n",
    "\n",
    "# Data loading parameters\n",
    "params = {'batch_size': 1, 'shuffle': True, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Task actions names\n",
    "task_labels = ['JumpForward', 'JumpForward', 'Run', 'Run', 'TurnLeft', 'TurnLeft', 'TurnRight', 'TurnRight']\n",
    "# task_labels = ['JumpForward', 'Run', 'TurnLeft', 'TurnRight']\n",
    "\n",
    "# convert labels -> category\n",
    "le = LabelEncoder()\n",
    "le.fit(task_labels)\n",
    "\n",
    "# show how many classes there are\n",
    "list(le.classes_)\n",
    "\n",
    "# convert category -> one-hot\n",
    "action_category = le.transform(task_labels).reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(action_category)\n",
    "\n",
    "tasks = []\n",
    "fnames = os.listdir(data_path)\n",
    "\n",
    "all_tasks_names = []\n",
    "for f in fnames:\n",
    "    loc1 = f.find('v_')\n",
    "    loc2 = f.find('_g')\n",
    "    tasks.append(f[(loc1 + 2) : loc2])\n",
    "\n",
    "    all_tasks_names.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data files\n",
    "x_list = all_tasks_names              # all video file names\n",
    "y_list = le.transform(task_labels)    # all video labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random split sample set to training set and test set\n",
    "train_list, test_list, train_label, test_label = train_test_split(x_list, y_list, test_size=0.25, random_state=42)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize([img_x, img_y]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "selected_frames = np.arange(b_frame, e_frame, s_frame).tolist()\n",
    "\n",
    "train_set, valid_set = Dataset_CRNN(data_path, train_list, train_label, selected_frames, transform=transform) ,\\\n",
    "                       Dataset_CRNN(data_path, test_list, test_label, selected_frames, transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, **params)\n",
    "valid_loader = data.DataLoader(valid_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "cnn = CNN().to(device)\n",
    "lstm = LSTM().to(device)\n",
    "\n",
    "# Parallelize model to multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    cnn_encoder = nn.DataParallel(cnn)\n",
    "    rnn_decoder = nn.DataParallel(lstm)\n",
    "\n",
    "crnn_params = list(cnn.parameters()) + list(lstm.parameters())\n",
    "optimizer = torch.optim.Adam(crnn_params, lr=learning_rate)\n",
    "\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- Start training and save results by epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores = train(log_interval, [cnn, lstm], device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation([cnn, lstm], device, optimizer, valid_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "    epoch_test_scores.append(epoch_test_score)\n",
    "\n",
    "    # save all train test results\n",
    "    A, B, C, D = np.array(epoch_train_losses), np.array(epoch_train_scores), \\\n",
    "                 np.array(epoch_test_losses), np.array(epoch_test_scores)\n",
    "    np.save('./outputs/CRNN_epoch_training_losses.npy', A)\n",
    "    np.save('./outputs/CRNN_epoch_training_scores.npy', B)\n",
    "    np.save('./outputs/CRNN_epoch_test_loss.npy', C)\n",
    "    np.save('./outputs/CRNN_epoch_test_score.npy', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GAIL (Todo)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}